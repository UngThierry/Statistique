{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "LIGNE = 6\r\n",
    "COLONNE = 7\r\n",
    "a = 1\r\n",
    "print(a)\r\n",
    "a = COLONNE\r\n",
    "print(a)\r\n",
    "a = COLONNE * (1 * COLONNE + LIGNE * (0))\r\n",
    "print(a)\r\n",
    "a = COLONNE * (1 * COLONNE + LIGNE * (7/2 +1))\r\n",
    "print(a)\r\n",
    "a = COLONNE * (1 * COLONNE + LIGNE * (49/2 + 1))\r\n",
    "print(a)\r\n",
    "a = COLONNE * (1 * COLONNE + LIGNE * (238/3 + 21 ))\r\n",
    "print(a)\r\n",
    "a = COLONNE * (1 * COLONNE + LIGNE * (1120/3 + 14))\r\n",
    "print(a)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "7\n",
      "49\n",
      "238.0\n",
      "1120.0\n",
      "4263.0\n",
      "16317.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import numpy as np\r\n",
    "import random\r\n",
    "from math import *\r\n",
    "from gameFunctions import put_data_in_tab\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "class Bandits:\r\n",
    "    #static Random levers bernoulli parameter\r\n",
    "    leversRand = np.random.sample(size = 10)\r\n",
    "    #static epsilon\r\n",
    "    eps = 0.4\r\n",
    "    \r\n",
    "    def __init__(self,nbLevers,nbIt,muVals = \"random\",eps=0.4,algo=\"baseline\"):\r\n",
    "        self.nbLevers = nbLevers\r\n",
    "        self.nbIt = nbIt\r\n",
    "        if((type(muVals).__module__ == np.__name__) or type(muVals) == list):\r\n",
    "            self.muVals = np.array(muVals)\r\n",
    "        else:\r\n",
    "            self.muVals = np.random.sample(size = nbLevers)\r\n",
    "        self.eps = eps\r\n",
    "        self.t = 0\r\n",
    "        algo = algo.lower()\r\n",
    "        if(algo == \"greedy\" or algo == \"greedyeps\" or algo == \"ucb\"):\r\n",
    "            self.algo = algo\r\n",
    "        else:\r\n",
    "            self.algo = \"baseline\"\r\n",
    "        #count the reward -> useful to calculate the regret for the loss fonction\r\n",
    "        self.reward = 0\r\n",
    "    \r\n",
    "    def binaryReward(self,levers,action):\r\n",
    "        p = random.random()\r\n",
    "        #print(\"Action mu is \"+str(levers[action]))\r\n",
    "        #print(\"Result is \"+str(p))\r\n",
    "        if(p<levers[action]):\r\n",
    "            return 1\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "            \r\n",
    "    def baseline(self,meanLeverRewards,nbOfLeverUse):\r\n",
    "        a = floor(random.random()*self.nbLevers)\r\n",
    "        reward = self.binaryReward(self.muVals,a)\r\n",
    "        nbOfLeverUse[a] += 1\r\n",
    "        meanLeverRewards[a] = (meanLeverRewards[a]*(nbOfLeverUse[a]-1) + reward) / nbOfLeverUse[a]\r\n",
    "        return {\"action\":a,\"reward\":reward,\"meanLeverRewards\":meanLeverRewards,\"nbOfLeverUse\":nbOfLeverUse}\r\n",
    "    \r\n",
    "\r\n",
    "    def greedy(self,meanLeverRewards, nbOfLeverUse):\r\n",
    "        leverReward = np.zeros(self.nbLevers)\r\n",
    "        for i in range(self.nbLevers):\r\n",
    "            leverReward[i] += self.binaryReward(self.muVals,i)\r\n",
    "            nbOfLeverUse[i]+=1\r\n",
    "        for i in range(self.nbLevers):\r\n",
    "            meanLeverRewards[i] = leverReward[i] / nbOfLeverUse[i]\r\n",
    "        a = meanLeverRewards.argmax()\r\n",
    "        reward = self.binaryReward(self.muVals,a)\r\n",
    "        nbOfLeverUse[a] += 1\r\n",
    "        meanLeverRewards[a] = (meanLeverRewards[a]*(nbOfLeverUse[a]-1) + reward) / nbOfLeverUse[a]\r\n",
    "        return {\"action\":a,\"reward\":reward,\"meanLeverRewards\":meanLeverRewards,\"nbOfLeverUse\":nbOfLeverUse}\r\n",
    "    \r\n",
    "    def greedyEps(self,meanLeverRewards, nbOfLeverUse):\r\n",
    "        p = random.random()\r\n",
    "        if(p < self.eps):\r\n",
    "            return self.baseline(meanLeverRewards, nbOfLeverUse)\r\n",
    "        else:\r\n",
    "            return self.greedy(meanLeverRewards, nbOfLeverUse)\r\n",
    "\r\n",
    "    def ucb(self,meanLeverRewards, nbOfLeverUse):\r\n",
    "        res = meanLeverRewards + np.sqrt(2*np.log10(self.t)/nbOfLeverUse)\r\n",
    "        a = np.argmax(res)\r\n",
    "        reward = self.binaryReward(self.muVals,a)\r\n",
    "        nbOfLeverUse[a] += 1\r\n",
    "        meanLeverRewards[a] = (meanLeverRewards[a]*(nbOfLeverUse[a]-1) + reward) / nbOfLeverUse[a]\r\n",
    "        return {\"action\":a,\"reward\":reward,\"meanLeverRewards\":meanLeverRewards,\"nbOfLeverUse\":nbOfLeverUse}\r\n",
    "        \r\n",
    "    def pullLever(self,meanLeverRewards,nbOfLeverUse):\r\n",
    "        self.t += 1\r\n",
    "        if(self.algo == \"baseline\"):\r\n",
    "            return self.baseline(meanLeverRewards,nbOfLeverUse)\r\n",
    "        elif(self.algo == \"greedy\"):\r\n",
    "            return self.greedy(meanLeverRewards,nbOfLeverUse)\r\n",
    "        elif(self.algo == \"greedyeps\"):\r\n",
    "            return self.greedyEps(meanLeverRewards,nbOfLeverUse)\r\n",
    "        elif(self.algo == \"ucb\"):\r\n",
    "            return self.ucb(meanLeverRewards,nbOfLeverUse)\r\n",
    "\r\n",
    "    def optimalReward(self):\r\n",
    "        return self.nbIt*np.max(self.muVals)\r\n",
    "\r\n",
    "\r\n",
    "    def run(self):\r\n",
    "        meanLeverRewards = np.zeros(len(self.muVals))\r\n",
    "        #Start count at 1 to avoid divide by 0\r\n",
    "        nbOfLeverUse = np.ones(len(self.muVals))\r\n",
    "        regretTab = []\r\n",
    "        for i in range(self.nbIt):\r\n",
    "            pullRes = self.pullLever(meanLeverRewards,nbOfLeverUse)\r\n",
    "            meanLeverRewards = pullRes[\"meanLeverRewards\"]\r\n",
    "            nbOfLeverUse = pullRes[\"nbOfLeverUse\"]\r\n",
    "            self.reward += pullRes[\"reward\"]\r\n",
    "            #print(self.reward)\r\n",
    "            regretTab.append(self.optimalReward() - self.reward)\r\n",
    "        regret = self.optimalReward()-self.reward\r\n",
    "        accuracy = (self.reward / self.optimalReward())\r\n",
    "        return {\"reward\":self.reward,\"optimalReward\":self.optimalReward(),\"regret\":regret,\"accuracy\":accuracy,\"regretTab\":regretTab}\r\n",
    "    \r\n",
    "def writeInFile():\r\n",
    "    fBaseline = open(\"Baseline1.txt\",\"w\")\r\n",
    "    fGreedy = open(\"Greedy1.txt\",\"w\")\r\n",
    "    fGreedyEps = open(\"GreedyEps1.txt\",\"w\")\r\n",
    "    fUcb = open(\"UCB1.txt\",\"w\")\r\n",
    "    fOptimal = open(\"Optimal1.txt\",\"w\")\r\n",
    "    greedy = []\r\n",
    "    greedyEps = []\r\n",
    "    ucb = []\r\n",
    "    #create a linspace\r\n",
    "    levers = np.linspace(0.0,1.0,100,endpoint = 0)\r\n",
    "    #shuffle values\r\n",
    "    np.random.shuffle(levers)\r\n",
    "    a = Bandits(len(levers),50000,algo=\"baseline\",muVals = levers)\r\n",
    "    print(\"Baseline\")\r\n",
    "    res = a.run()\r\n",
    "    baseline = res[\"regretTab\"]\r\n",
    "    a = Bandits(len(levers),50000,algo=\"greedy\",muVals = levers)\r\n",
    "    print(\"Greedy\")\r\n",
    "    res = a.run()\r\n",
    "    greedy = res[\"regretTab\"]\r\n",
    "    a = Bandits(len(levers),50000,algo=\"greedyEps\",muVals = levers)\r\n",
    "    print(\"GreedyEps\")\r\n",
    "    res = a.run()\r\n",
    "    greedyEps = res[\"regretTab\"]\r\n",
    "    a = Bandits(len(levers),50000,algo=\"ucb\",muVals = levers)\r\n",
    "    print(\"UCB\")\r\n",
    "    res = a.run()\r\n",
    "    ucb = res[\"regretTab\"]\r\n",
    "\r\n",
    "    for i in range(50000):\r\n",
    "        fBaseline.write(str(baseline[i]) + \"\\n\")\r\n",
    "        fGreedy.write(str(greedy[i]) + \"\\n\")\r\n",
    "        fGreedyEps.write(str(greedyEps[i]) + \"\\n\")\r\n",
    "        fUcb.write(str(ucb[i]) + \"\\n\")\r\n",
    "        fOptimal.write(str(levers.max()*50000)+ \"\\n\")\r\n",
    "    print(\"Written\")\r\n",
    "\r\n",
    "    fBaseline.close()\r\n",
    "    fGreedy.close()\r\n",
    "    fGreedyEps.close()\r\n",
    "    fUcb.close()\r\n",
    "    fOptimal.close()\r\n",
    "\r\n",
    "\r\n",
    "#writeInFile()\r\n",
    "f1 = open(\"Baseline1.txt\",\"r\")\r\n",
    "f2 = open(\"Greedy1.txt\",\"r\")\r\n",
    "f3 = open(\"GreedyEps1.txt\",\"r\")\r\n",
    "f4 = open(\"UCB1.txt\",\"r\")\r\n",
    "tabP1 = put_data_in_tab(f1)\r\n",
    "tabP2 = put_data_in_tab(f2)\r\n",
    "tabP3 = put_data_in_tab(f3)\r\n",
    "tabP4 = put_data_in_tab(f4)\r\n",
    "x = np.linspace(1,50000,50000)\r\n",
    "f1.close()\r\n",
    "f2.close()\r\n",
    "f3.close()\r\n",
    "f4.close()\r\n",
    "plt.plot(x,tabP1, label =\"BaselineRegret\")\r\n",
    "plt.plot(x,tabP2, label =\"GreedyRegret\")\r\n",
    "plt.plot(x,tabP3, label =\"GreedyEpsRegret\")\r\n",
    "plt.plot(x,tabP4, label =\"UCBRegret\")\r\n",
    "plt.xlabel(\"Time\")\r\n",
    "plt.ylabel(\"Regret\")\r\n",
    "plt.legend()\r\n",
    "plt.grid(lw = 0.25)\r\n",
    "plt.title('Regret as a function of time')\r\n",
    "plt.savefig('Regret.png')\r\n",
    "plt.show()\r\n",
    "'''\r\n",
    "#create a linspace\r\n",
    "levers = np.linspace(0.0,1.0,100,endpoint = 0)\r\n",
    "#shuffle values\r\n",
    "np.random.shuffle(levers)\r\n",
    "a = Bandits(len(levers),50000,algo=\"baseline\",muVals = levers)\r\n",
    "loss = a.run()\r\n",
    "print(loss)\r\n",
    "print(a.muVals)\r\n",
    "'''"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "9c6bd1c1dcf038e17a85a01199468631d2d406fade8f6cc5eea24dc919f509a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}